# -*- coding: utf-8 -*-
"""「embedding2.ipynb」的副本

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/117UCGb3u1qRiUxandc4WWFugw1zfsefh

## The IMDB dataset

We will work with the IMDB dataset, which contains 25,000 movie reviews from [IMDB](https://www.imdb.com/). Each review is labeled as positive or negative from the rating provided by users together with their reviews. 

The dataset is distributed with keras. Before loading it in memory, we initialize our tools:

## Load data & preprocess
"""

# get reproducible results
from numpy.random import seed
seed(0xdeadbeef)
import tensorflow as tf
tf.random.set_seed(0xdeadbeef)

from tensorflow import keras
imdb = keras.datasets.imdb
num_words = 20000
(train_data, train_labels), (test_data, test_labels) = imdb.load_data(seed=1, num_words=num_words)

valid_data = test_data[:12500]
test_data = test_data[12500:]

valid_labels = test_labels[:12500]
test_labels = test_labels[12500:]

max=0
for i in range(len(train_data)):
  if len(train_data[i]) > max:
    max = len(train_data[i])

max

import pandas as pd
comment = pd.DataFrame({"train_data":train_data, "train_labels":train_labels})
comment.head()

comment.info()

comment.train_labels.value_counts()

comment_test = pd.DataFrame({"test_data":test_data, "test_labels":test_labels})
comment_test[:100]

comment_test.info()

comment_test.test_labels.value_counts()

len(train_data), len(test_data), len(valid_data)

train_data.shape, train_labels.shape

train_data[0][:10], train_labels[0]

"""At the moment, `imdb.load_data` triggers deprecation warnings that can be ignored.

The first review looks like this:
"""

print(train_data[0])
print('label:', train_labels[0])

"""We see that the text of the review has been encoded as a sequence of integers. Please refer to [part 2](https://thedatafrog.com/text-preprocessing-machine-learning-yelp/) of this tutorial series if you want to understand how such an encoding can be done in practice. 

Each word in the text is represented as an integer. A dictionary called the **vocabulary** links each word to a unique integer. In the example above, we see that the integer 4 is repeated many times. This integer corresponds to a very frequent word. And actually, the more frequent a word, the lower the integer.    

To decode the review, we need to make use of the vocabulary:
"""

# A dictionary mapping words to an integer index
vocabulary = imdb.get_word_index()

# The first indices are reserved
vocabulary = {k:(v+3) for k,v in vocabulary.items()} 
vocabulary["<pad>"] = 0
# See how integer 1 appears first in the review above. 
vocabulary["<sos>"] = 1
vocabulary["<unk>"] = 2  # unknown
vocabulary["<unused>"] = 3

# reversing the vocabulary. 
# in the index, the key is an integer, 
# and the value is the corresponding word.
index = dict([(value, key) for (key, value) in vocabulary.items()])

def decode_review(text):
    '''converts encoded text to human readable form.
    each integer in the text is looked up in the index, and 
    replaced by the corresponding word.
    '''
    return ' '.join([index.get(i, '?') for i in text])

decode_review(train_data[0])

"""We see that integer 4 indeed corresponds to a very frequent word, "the". Now what do we do with this dataset? We can see two issues if we are to use it as input to a neural network:

* The reviews have a variable number of words, while the network has a fixed number of neurons.
* The words are completely independent. For example, "brilliant" and "awesome" correspond to two different integers, and the neural network does not know a priori that these two adjectives have similar meaning. 

Let's deal with the first issue. To get a fixed length input, we can simply truncate the reviews to a fixed number of words, say 256. For reviews that have more than 256 words, we will keep only the first 256 words. For shorter reviews, we will fill the unused word slots with zeros. With keras, this is easy to do:
"""

# fixed every text length
fixed_length=1000
train_data = keras.preprocessing.sequence.pad_sequences(train_data,
                             value=vocabulary["<pad>"],
                             padding='post',
                             maxlen=fixed_length)

valid_data = keras.preprocessing.sequence.pad_sequences(valid_data,
                            value=vocabulary["<pad>"],
                            padding='post',
                            maxlen=fixed_length)
test_data = keras.preprocessing.sequence.pad_sequences(test_data,
                            value=vocabulary["<pad>"],
                            padding='post',
                            maxlen=fixed_length)

train_data[1]

import pandas as pd
import matplotlib.pyplot as plt
from matplotlib.pyplot import MultipleLocator

# Plot the validation and training data separately
def plot_loss_curves(history):
  """
  Returns separate loss curves for training and validation metrics.
  """ 
  loss = history.history['loss']
  val_loss = history.history['val_loss']

  accuracy = history.history['binary_accuracy']
  val_accuracy = history.history['val_binary_accuracy']

  epochs = range(len(history.history['loss']))

  # 設置統一刻度間距及範圍
  # x_major_locator = MultipleLocator(1)
  # y_major_locator = MultipleLocator(0.05)
  
  # ax=plt.gca()
  # ax.xaxis.set_major_locator(x_major_locator)
  # ax.yaxis.set_major_locator(y_major_locator)
  # plt.xlim(0, 10)
  pd.DataFrame(history.history).plot(figsize=(10,7));

  # Plot loss
  plt.figure()
  plt.plot(epochs, loss, label='training_loss')
  plt.plot(epochs, val_loss, label='val_loss')
  plt.title('Loss')
  plt.xlabel('Epochs')
  plt.legend()

  # Plot accuracy
  plt.figure()
  plt.plot(epochs, accuracy, label='training_accuracy')
  plt.plot(epochs, val_accuracy, label='val_accuracy')
  plt.title('Accuracy')
  plt.xlabel('Epochs')
  plt.legend();['val_loss']

"""The first issue is solved: Our reviews are now of fixed length. The second issue is addressed below. 

## Embedding: Why is it Needed? 

In this section, we will see why a standard dense neural network is by essence unable to deal with this dataset.

As we have seen in my post about [the 1-neuron network](https://thedatafrog.com/logistic-regression/), neural networks are simply a function of their inputs. In the 1-neuron network (or logistic regression), this function is 

$$ f( \{ x_i \} ) = \sigma (z)$$

where $\sigma$ is the sigmoid function and where the weighted input $z$ of the only neuron is computed as 

$$z= b+ \sum_{i} w_i x_i$$ 

In this expression, the sum runs over all input variables $x_i$, and $b$ is the bias of the neuron. When the network is trained, the network parameters (the biases and weights) are tuned for best performance. 

Now what would happen if we tried to classify our reviews as either positive or negative with a logistic regression?

The input variables would be the 256 integers in the review array. Let us assume a positive weight $w_i$ for variable i. Since the sigmoid function increases monotonically, when variable $i$ increases, the output of the network, which is the probability for the review to be positive, increases.

But there is no reason for variable $i$ to be correlated in any way to the probability for the review to be positive! 

For example, variable $i$ could be 27 (corresponding to the word "bad"), 118 ("good"), or 1562 ("awful"). The review does not look more positive when the integer gets bigger. 
In other words, the actual value of the integer code for a given word does not carry any information by itself on the quality of the review. This value is arbitrary, and would be different with another encoding algorithm. 

Clearly, a logistic regression would not be of any use in classifying our reviews.

Maybe the logistic regression is too simple, and a more complex dense network would help? 

Actually, that's not the case. Indeed, in a dense network with hidden layers, each of the neurons in the first hidden layer behaves as the single neuron of the logistic regression. So more complex dense networks suffer from the same problems as the simple logistic regression. 


## Simple sentiment analysis with embedding

Embedding is a way to extract the meaning of a word. In the embedding process, each word (or more precisely, each integer corresponding to a word) is translated to a vector in N-dimensional space. 

That does sound complicated! but it's not.  

To understand better, we're going to perform the embedding in two dimensions only. Each word is then going to be converted to a vector with two coordinates $(x,y)$, and these coordinates can be represented as a point in a plane. 

As you will see below, the text of a review will appear as an ensemble of points, and the points with similar meaning will be grouped together. Looking at the distribution of the points in 2D, the neural network will be able to predict whether the review is positive or negative. 

In practice, here is how to do that in keras. If anything is unclear in the steps below, you might want to look at my post about [deep learning for image recognition with keras](https://thedatafrog.com/deep-learning-keras/).

## Model 1
"""

model = keras.Sequential()

# the first layer is the embedding layer. 
# we indicate the number of possible words, 
# the dimension of the embedding space, 
# and the maximum size of the text. 
model.add(keras.layers.Embedding(len(vocabulary), 30, input_length=256))

# the output of the embedding is multidimensional, 
# with shape (256, 30)
# for each word, we obtain 30 values, 
# the x and y coordinates
# we flatten this output to be able to 
# use it in a dense layer
# model.add(keras.layers.Flatten())
model.add(keras.layers.SimpleRNN(10))

# dropout regularization
# model.add(keras.layers.Dropout(rate=0.5))

# small dense layer. It's role is to analyze 
# the distribution of points from embedding
model.add(keras.layers.Dense(5))

# final neuron, with sigmoid activation 
# for binary classification
model.add(keras.layers.Dense(1, activation='sigmoid'))

model.summary()

# model = keras.Sequential()
# model.add(keras.layers.Embedding(len(vocabulary), 30, input_length=256))
# model.add(keras.layers.SimpleRNN(10))
# model.add(keras.layers.Dropout(rate=0.5))
# model.add(keras.layers.Dense(5))
# model.add(keras.layers.Dense(1, activation='sigmoid'))

from tensorflow.keras.utils import plot_model
plot_model(model = model, show_shapes = True)

"""As you can see, it is enough to add an embedding layer to our neural network to perform embedding. And it could be a bit surprising to see that we only have to provide three arguments to construct the embedding layer: the length of the vocabulary, the number of dimensions in the embedding space, and the number of words in the input text. 

In the summary printout just above, we see that the embedding layer represents 177176 parameters. At first, these parameters are random, and the embedding is just meaningless. During the training, these parameters are tuned and the network becomes more and more capable to extract the meaning of the words.

Now that the model is ready, we compile it and we train it. There is no need for a GPU here, as the training will be quite fast even on a CPU.
"""

model.compile(optimizer='adam',
       loss='binary_crossentropy',
       metrics=['accuracy'])

callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)

history = model.fit(train_data,
           train_labels,
           epochs=20,
           batch_size=100,
           validation_data=(valid_data, valid_labels),
           callbacks = [callback]
           )

model_pred = model.predict(test_data)
model_pred

model_pred = tf.squeeze(model_pred)

model_pred.shape, test_labels.shape

model_pred

test_labels

loss, acc = model.evaluate(test_data, test_labels)
loss, acc

"""We can now look at the evolution of the training and testing accuracies as a function of time, with this small function: """

# evaluate model
import pandas as pd
pd.DataFrame(history.history).plot(figsize=(10,7));

import matplotlib.pyplot as plt
plot_loss_curves(history)

# import matplotlib.pyplot as plt
# def plot_accuracy(history, miny=None):
#   acc = history.history['accuracy']
#   val_acc = history.history['val_accuracy']
#   epochs = range(len(acc))
#   plt.plot(epochs, acc)
#   plt.plot(epochs, val_acc)
#   if miny:
#     plt.ylim(miny, 1.0)
#   plt.title('accuracy') 
#   plt.xlabel('epoch')
#   plt.figure()

# plot_accuracy(history)

"""## Model_1"""

# 減少 embedding 維度到 20, 且增加一層 rnn layer, 第一層 rnn 讓他 return sequence, 倒數第二層dense 改relu
model_1 = keras.Sequential()
model_1.add(keras.layers.Embedding(len(vocabulary), 20, input_length=256))
model_1.add(keras.layers.SimpleRNN(10, return_sequences=True))
model_1.add(keras.layers.SimpleRNN(10))
# model_1.add(keras.layers.Dropout(rate=0.5))
model_1.add(keras.layers.Dense(5, activation="relu"))
model_1.add(keras.layers.Dense(1, activation="sigmoid"))

model_1.compile(optimizer='adam',
       loss='binary_crossentropy',
       metrics=['accuracy'])
callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)

history_1 = model.fit(train_data,
           train_labels,
           epochs=20,
           batch_size=100,
           validation_data=(valid_data, valid_labels),
           callbacks = [callback])

loss_1, acc_1 = model_1.evaluate(test_data, test_labels)
loss_1, acc_1

import matplotlib.pyplot as plt
plot_loss_curves(history_1)

"""## Model_2"""

# model 過於複雜，減少一層dense
model_2 = keras.Sequential()
model_2.add(keras.layers.Embedding(len(vocabulary), 20, input_length=256))
model_2.add(keras.layers.SimpleRNN(10, return_sequences=True))
model_2.add(keras.layers.SimpleRNN(10))
# model_1.add(keras.layers.Dropout(rate=0.5))
# model_1.add(keras.layers.Dense(5, activation="relu"))
model_2.add(keras.layers.Dense(1, activation="sigmoid"))

model_2.compile(optimizer='adam',
       loss='binary_crossentropy',
       metrics=['accuracy'])
callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)

history_2 = model.fit(train_data,
           train_labels,
           epochs=20,
           batch_size=100,
           validation_data=(valid_data, valid_labels),
           callbacks = [callback])

loss_2, acc_2 = model_2.evaluate(test_data, test_labels)
loss_2, acc_2

import matplotlib.pyplot as plt
plot_loss_curves(history_2)

"""## Model_2"""

# 
model_2 = keras.Sequential()
model_2.add(keras.layers.Embedding(len(vocabulary), 20, input_length=256))
model_2.add(keras.layers.SimpleRNN(10))
# model_2.add(keras.layers.SimpleRNN(10))
# model_1.add(keras.layers.Dropout(rate=0.5))
model_2.add(keras.layers.Dense(5, activation="relu"))
model_2.add(keras.layers.Dense(1, activation="sigmoid"))

model_2.compile(loss=tf.keras.losses.binary_crossentropy,
        optimizer=tf.optimizers.SGD(learning_rate=0.001),
        metrics=["accuracy"])
callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)

history_2 = model_2.fit(train_data,
           train_labels,
           epochs=20,
           batch_size=100,
           validation_data=(valid_data, valid_labels),
           callbacks = [callback]
           )

loss_2, acc_2 = model_2.evaluate(test_data, test_labels)
loss_2, acc_2

import matplotlib.pyplot as plt
plot_loss_curves(history_2)

"""## Model_3"""

# add recurrent dropout
model_3 = keras.Sequential()
model_3.add(keras.layers.Embedding(len(vocabulary), 20, input_length=256))
model_3.add(keras.layers.SimpleRNN(10, recurrent_dropout=0.5))

# model_2.add(keras.layers.SimpleRNN(10))
# model_1.add(keras.layers.Dropout(rate=0.5))
model_3.add(keras.layers.Dense(5, activation="relu"))
model_3.add(keras.layers.Dense(1, activation="sigmoid"))

model_3.compile(loss=tf.keras.losses.binary_crossentropy,
        optimizer=tf.optimizers.SGD(learning_rate=0.001),
        metrics=["accuracy"])
callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)

history_3 = model_3.fit(train_data,
           train_labels,
           epochs=20,
           batch_size=100,
           validation_data=(valid_data, valid_labels),
           callbacks = [callback]
           )

import matplotlib.pyplot as plt
plot_loss_curves(history_3)

"""## Model_4"""

# try layernorm
model_4=keras.Sequential([
  keras.layers.Embedding(len(vocabulary), 20, input_length=256),
  keras.layers.SimpleRNN(10, recurrent_dropout=0.5),
  keras.layers.LayerNormalization(),
  keras.layers.Dense(5, activation="relu"),
  keras.layers.Dense(1, activation="sigmoid")
])

model_4.compile(loss="binary_crossentropy",
        optimizer=tf.optimizers.SGD(learning_rate=0.01),
        metrics=["accuracy"])
callback = tf.keras.callbacks.EarlyStopping(monitor="val_loss", patience=3)

history_4=model_4.fit(train_data, train_labels, epochs=20, validation_data=(valid_data, valid_labels), callbacks=[callback])

import matplotlib.pyplot as plt
plot_loss_curves(history_4)

"""## Model_5"""

# change to lstm layer
from tensorflow.keras.utils import plot_model
model_5 = tf.keras.Sequential([
  tf.keras.layers.Embedding(len(vocabulary), 20, input_length=256),
  tf.keras.layers.LSTM(10, dropout=0.5, return_sequences=True),
  tf.keras.layers.LSTM(10, dropout=0.5, return_sequences=True),
  
  tf.keras.layers.GlobalAveragePooling1D(),
  tf.keras.layers.Dense(1,activation="sigmoid")
])

model_5.compile(loss="binary_crossentropy",
        optimizer=tf.optimizers.SGD(learning_rate=0.001),
        metrics=["binary_accuracy"])
callback=tf.keras.callbacks.EarlyStopping(monitor="val_loss", patience=3)


plot_model(model = model_5, show_shapes = True)

history_5 = model_5.fit(train_data, train_labels, epochs=20, validation_data=(valid_data, valid_labels), callbacks=[callback])

loss_5, acc_5 = model_5.evaluate(test_data, test_labels)
loss_5, acc_5

model_5.save("imdb_model_5")

import matplotlib.pyplot as plt
plot_loss_curves(history_5)

"""============> 做資料視覺化、及 metric 定義 <=======================

## Model_6
"""

len(vocabulary)

# change to lstm layer
from tensorflow.keras.utils import plot_model

model_6 = tf.keras.Sequential([
  tf.keras.layers.Embedding(len(vocabulary), 20, input_length=256),
  tf.keras.layers.LSTM(10, dropout=0.5, return_sequences=True),
  tf.keras.layers.GlobalAveragePooling1D(),
  tf.keras.layers.Dense(1,activation="sigmoid")
])

model_6.compile(loss="binary_crossentropy",
        optimizer=tf.optimizers.Adam(learning_rate=0.001),
        metrics=["binary_accuracy"])
callback=tf.keras.callbacks.EarlyStopping(monitor="val_loss", patience=3)


plot_model(model = model_6, show_shapes = True)

model_6.summary()

history_6 = model_6.fit(train_data, train_labels, epochs=20, validation_data=(valid_data, valid_labels), callbacks=[callback])

loss_6, acc_6 = model_6.evaluate(test_data, test_labels)
loss_6, acc_6

import matplotlib.pyplot as plt
plot_loss_curves(history_6)

model_6.save("imdb_model_6")

"""## Model_7"""

# change to GRU layer
from tensorflow.keras.utils import plot_model

model_7 = tf.keras.Sequential([
  tf.keras.layers.Embedding(len(vocabulary), 20, input_length=256),
  tf.keras.layers.GRU(10, dropout=0.5, return_sequences=True),
  tf.keras.layers.GlobalAveragePooling1D(),
  tf.keras.layers.Dense(1,activation="sigmoid")
])

model_7.compile(loss="binary_crossentropy",
        optimizer=tf.optimizers.Adam(learning_rate=0.001),
        metrics=["binary_accuracy"])
callback=tf.keras.callbacks.EarlyStopping(monitor="val_loss", patience=3)


plot_model(model = model_7, show_shapes = True)

model_7.summary()

history_7 = model_7.fit(train_data, train_labels, epochs=20, validation_data=(valid_data, valid_labels), callbacks=[callback])

loss_7, acc_7 = model_7.evaluate(test_data, test_labels)
loss_7, acc_7

import matplotlib.pyplot as plt
plot_loss_curves(history_7)

model_7.save("imdb_model_7")

"""## Model_8"""

# changing GRU layer to input_len=200
from tensorflow.keras.utils import plot_model

model_8 = tf.keras.Sequential([
  tf.keras.layers.Embedding(len(vocabulary), 30, input_length=1000),
  tf.keras.layers.GRU(10, dropout=0.5, return_sequences=True),
  tf.keras.layers.GRU(10, dropout=0.4, return_sequences=True),
  tf.keras.layers.GlobalAveragePooling1D(),
  tf.keras.layers.Dense(1,activation="sigmoid")
])

model_8.compile(loss="binary_crossentropy",
        optimizer=tf.optimizers.Adam(learning_rate=0.001),
        metrics=["binary_accuracy"])
callback=tf.keras.callbacks.EarlyStopping(monitor="val_loss", patience=3)


plot_model(model = model_8, show_shapes = True)

model_8.summary()

history_8 = model_8.fit(train_data, train_labels, epochs=20, validation_data=(valid_data, valid_labels))

loss_8, acc_8 = model_8.evaluate(test_data, test_labels)
print("loss: ",loss_8, " accuracy: ", acc_8)

import matplotlib.pyplot as plt
plot_loss_curves(history_8)

"""We are able to reach a classification accuracy over 85%. The training accuracy continuously increases while the training accuracy plateaus after the second epoch. This means that we [overfit](https://thedatafrog.com/overfitting-illustrated/). But in this post, that's none of our concern: we only want to understand  what is embedding. 

## Visualizing embedding

To investigate embedding, we create a function that provides the output of the embedding layer for a given input:
"""

# with a Sequential model
get_embed_out = keras.backend.function(
    [model.layers[0].input],
    [model.layers[1].output])

"""Then, we use this function to get the distribution of points from the embedding of the first review in the validation sample: """

layer_output = get_embed_out([test_data[0]])
print(type(layer_output), len(layer_output), layer_output[0].shape)

"""We see that layer_output is a list with a single element containing the array of embedded words. So let's get this array and plot it: """

words = layer_output[0]
plt.scatter(words[:,0], words[:,1])

"""We see that the distribution of embedded words for this review has an elongated shape. But what does it mean? 

Let's create our own review, encode it, and embed it to see what happens: 
"""

review = ['great', 'brilliant','crap','bad', 
          'fantastic', 'movie', 'seagal']
enc_review = tf.constant([vocabulary[word] for word in review])
enc_review

words = get_embed_out([enc_review])[0]

plt.scatter(words[:,0], words[:,1])
for i, txt in enumerate(review):
    plt.annotate(txt, (words[i,0], words[i,1]))

"""We see that words with a similar meaning are indeed grouped together. There is one group with "great", "brilliant", "fantastic". This area of the plane contains the words that carry a positive meaning. On the other side, we have "bad", "crap", and ... "seagal". It seems that hiring Steven Seagal for your movie guarantees bad reviews on IMDB! 

The word "movie", finally, does not carry much meaning about the quality of the review, and ends up in the middle.

**Exercise**: I found a bad actor, that's easy, especially with Steven around. Can you find a good one?  

Now let's plot the distribution of points for a few reviews with the following function: 
"""

import math
def plot_review(i):
    # plot the distribution of points
    enc_words = test_data[i]
    emb_words = get_embed_out([enc_words])[0]
    plt.figure(figsize=(8,8))
    plt.scatter(emb_words[:,0], emb_words[:,1])
    # use the label as title: 1 is positive, 
    # 0 is negative
    plt.title(test_labels[i])
    # for words that are far enough from (0,0), 
    # print the word
    for i, (enc_word, emb_word) in enumerate(zip(enc_words, emb_words)):
        word = index[enc_word]
        x, y = emb_word
        if math.sqrt(x**2 + y**2)>0.2: 
            plt.annotate(word, (x, y))
    # fix the range in x and y to be able to compare
    # the distributions of different reviews
    axes = plt.gca()
    axes.set_xlim([-0.5,0.5])
    axes.set_ylim([-0.5, 0.5])
    axes.set_aspect('equal', adjustable='box')

plot_review(15)

plot_review(17)

"""You can plot a few more reviews if you wish, by changing the review index in the commands above. 

We see that positive reviews tend to have words with a meaning similar to "great", "brilliant", and "fantastic", while negative reviews tend to have words with a meaning similar to "seagal".

The goal of the dense layer in our network is to summarize the information in the whole distribution of points for each review. 

Now let's [go back](https://thedatafrog.com/word-embedding-sentiment-analysis/#wrapup) and wrap up with an interesting discussion about Steven Seagal and embedding.
"""